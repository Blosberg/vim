import pandas as pd
# Identify data type:
type(var)

# In pandas create 1-D arrays (series) like lists

= read/ write =

# read
df = pd.read_csv( "WindowMapOutput_w200_s50_table_noNA.tsv" , sep="\t")

# write
df.to_csv(file_name, sep='\t')

= Series =

S = pd.Series( [1,2,3],
               index=["a","b","c"])
# You can then index, slice, etc. the same way as lists.
S.loc["a":"c"]
S.iloc[0:2]

   = processing data: counts, sorting, etc. =

# check frequencies of occurance of each index
S.value_counts()

# sort (if hierarchical, argument becomes list)
S.sort_values(["first","second",...] inplace = False )

# str. attributes are available to every Series
S.str.[contains/startswith/isnumeric]()

= Data Frames =

There is a [[numpy]] array inside every data frame.
Access it with .values
Alternative access:

# -- useful df functions --
# get dimensions with:
df.shape
# names of columns/rows:
df.columns
df.index

# get first, last n lines
df.head(n), df.tail(n)

# get set of statistical info (mean, std. dev, etc.)
df.describe()

# get Pearson correlation between cols
pd.DataFrame.corr(df)

from sklearn.cluster.bicluster import SpectralCoclustering


   = build a data frame from Series =

To create data frames from series, you can combine series, and pandas will match the keys up for you:

>>> X=pd.Series([1,2,3,4], index=["a","b","c","d"])
>>> Y=pd.Series([5,6,7,8], index=["a","c","d","e"])
>>> df=pd.DataFrame({"First":X,"Second":Y})
>>> df
   First  Second
a    1.0     5.0
b    2.0     NaN
c    3.0     6.0
d    4.0     7.0
e    NaN     8.0
# Note the missing fields in b, and e

   = index of the df =

# change the first column define the index of the df (previous index is lost)
df.set_index("First", inplace=True)

# reset indext to default integers
df.reset_index()

# sort it by that criteria
df.sort_index()

   = Access rows (+boolean select)=

df.loc["b"]  # gets rows (or columns) from labels.
df.iloc[2]   # gets rows (or columns) at positions (only takes integers).
df.ix[2,3]        # First tries to be like loc, but if it fails,
            # it then falls back to iloc

df.iloc[[0,4,5],[1]]
df.ix[0:2,0:5]
# N.B. ix will first try to match index explicitly (so if your indexes are
# named 0,1,2, then this will include all three rows.) but if your indexes
# have a different name (e.g. "first","second","third") then it wil
# automatically switch to integer indexing and will not include the last
# value (i.e. 0:2 = [0,1])

   = Access columns =
# get col names:
list(df)


df["First"] or df.First
# Latter only works if the col name has no spaces.

# Access multiple cols by creating a list [ , ]
# within the dereferencing braces -hence, double braces
`df[["First","Second"]]`
df.index    # to get list a-f

   = clean out NaNs =

   # boolean col denoting whether the number of nans discovered in each row is 0
   Row_has_no_col_null = Prom_bc_paired.loc[:,["promoter","barcode"]].isnull().astype(int).sum(axis=1) == 0
   # filtered df
    Prom_bc_nunulls = Prom_bc_paired.loc[ Row_has_no_col_null, :]

   = Tidy, rearrange, melt =
# puts a table into single-observable column-wise format
# e.g. Starting off with a metilene output file (converted to tsv)
>>> df
	                q-value  ... p_2DKS 	mean_g1 	mean_g2
location 			 ...
chr1:7621909-7622017 	0.004158 ... 6.421400e-07 	0.698760 	0.50662
chr1:9680142-9680354 	0.023021 ... 8.969600e-06 	0.194800 	0.31956
chr1:14242645-14242713 	0.007769 ... 1.750300e-06 	0.054915 	0.18143
... 	... 	... 	... 	... 	... 	... 	...

# "Melt" into single-observable rows with:
>>>df.loc[:, [ "mean_g1", "mean_g2"] ].melt(var_name='AgeGroup', value_name='MethFrac')

  	AgeGroup 	MethFrac
0 	mean_g1 	0.698760
1 	mean_g1 	0.194800
2 	mean_g1 	0.054915
3 	mean_g1 	0.111400
4 	mean_g1 	0.280900
... 	... 	...


   = Groupby =

# Group by values with the same ColName
gb = df.groupby("ColName")
# Then gb is a set of tuples with the 1rst entry the ColName
# group label, and  the second being a dataframe (subset of the original).

for x,y in gb:
   # x is the ColName label, y is the group:
   ...

# look at groups
gb.groups
gb.get_group("Name") # specific group

# calculations on other cols within groups
gb["col"].agg(np.mean)

# batch calculations on the groupby sets:
gb.size()  # num entries in each group
gb.count() # ^ Same as above but num entries preserved in cols

gb.first()/last()     # first/last entry in each group
gb.mean()/min()/max() #

   == stack() ==
# Return a taller df
# stack Column subset --> into hierarchical index labels for rows (within  existing row index labels)

   == unstack() ==
# return a wider df
# unstack hierarchical index labels into Column subsets

# N.B. In both stacking, unstacking you can't go lower than the first set of
# columns and rows. This is just for pivotting subgroups within those
# columns/rows.

  == agg()  ==

# apply a named func to all remaining columns
  gb.agg(["min","max",...])

# Or supply a dictionary with specific funcs to each col:

  gb.agg({"Colname2":["func1","func2",...], "ColName3":... })
# OTHER cols   ^                              ^
# various funcs         ^        ^                      ^
# here, while groupby is grouped by ColName, you can operated on ColName2,3
# using functions specified in the corresponding dict.


= Boolean indexing =
AND : &
OR  : |
NOT : ~

df[(df.First <3.5) & (df.Fourth > 4 )]
# also: df.colname.str.contains("substring")-->bool

= Sort =
# sort by multiple hierarchical criteria:
df.sort_values( by=["most-important col",
                   "Second-most-important col",
                    ...],
               ascending = True [default] )
= TODO: =
* - Figure out "Categorical Indices" in data frames --Why can't you just add one? (see "flights" problem in 06_04 exercises)
* - How to use pd.IndexSlice to slice sub-indices (see 06_04)


